experiment_name: "distilbert_fast_v1"
description: "DistilBERT for faster training and inference with comparable performance"
tags: ["distilbert", "fast", "efficiency"]

model:
  architecture: "distilbert"
  model_name: "distilbert-base-uncased"
  num_labels: 3
  dropout: 0.1
  freeze_layers: 0

training:
  batch_size: 32  # Larger batch size due to smaller model
  eval_batch_size: 128
  learning_rate: 3.0e-5  # Slightly higher LR for DistilBERT
  epochs: 4  # More epochs to compensate for smaller model
  warmup_steps: 300
  weight_decay: 0.01
  max_grad_norm: 1.0
  scheduler: "linear"
  optimizer: "adamw"

data:
  dataset: "babe"
  train_split: 0.8
  preprocessing: "none"
  max_length: 512
  augmentation: false
  balance_classes: false
  filter_no_agreement: false

logging:
  tensorboard: true
  wandb: false
  log_steps: 50
  eval_steps: 250
  save_steps: 500
  log_model_graph: true
  log_confusion_matrix: true

cloud:
  platform: "gcp"
  machine_type: "n1-standard-2"  # Smaller machine for DistilBERT
  gpu_type: "nvidia-tesla-v100"
  gpu_count: 1
  preemptible: true
  region: "us-central1"
  zone: "us-central1-a"