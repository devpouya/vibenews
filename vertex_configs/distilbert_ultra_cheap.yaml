# Ultra Cheap DistilBERT Configuration (~$0.40 total cost)
experiment_name: "distilbert_ultra_cheap"
description: "Cheapest possible training with DistilBERT on minimal resources"
tags: ["ultra-cheap", "distilbert", "cost-optimized"]

model:
  architecture: "distilbert"
  model_name: "distilbert-base-uncased"
  num_labels: 3
  dropout: 0.1
  freeze_layers: 3  # Freeze half the layers to speed up training

training:
  batch_size: 32  # Larger batch size for efficiency
  eval_batch_size: 64
  learning_rate: 3.0e-5  # Slightly higher LR for faster convergence
  epochs: 2  # Fewer epochs to reduce cost
  warmup_steps: 200  # Shorter warmup
  weight_decay: 0.01
  max_grad_norm: 1.0
  scheduler: "linear"
  optimizer: "adamw"
  gradient_accumulation_steps: 1
  fp16: true  # Mixed precision for speed

data:
  dataset: "babe"
  train_split: 0.8
  preprocessing: "none"
  max_length: 256  # Shorter sequences for speed
  data_source: "gcs"
  filter_no_agreement: true  # Use only clear bias labels

logging:
  tensorboard: true
  log_steps: 50
  eval_steps: 200
  save_steps: 400
  log_confusion_matrix: true
  log_predictions: false  # Disable to save time

vertex:
  machine_type: "n1-standard-4"  # GPU-compatible machine type
  accelerator_type: "NVIDIA_TESLA_T4"
  accelerator_count: 1
  disk_type: "pd-standard"  # Cheaper disk
  disk_size_gb: 50  # Minimal disk size
  preemptible: true  # 70% cost reduction
  max_replica_count: 1