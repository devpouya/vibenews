# CPU-Only DistilBERT Configuration (no GPU quota needed)
experiment_name: "distilbert_cpu_only"
description: "CPU-only training with DistilBERT (no GPU quota required)"
tags: ["cpu-only", "distilbert", "no-gpu"]

model:
  architecture: "distilbert"
  model_name: "distilbert-base-uncased"
  num_labels: 3
  dropout: 0.1
  freeze_layers: 4  # Freeze more layers for faster CPU training

training:
  batch_size: 16  # Smaller batch for CPU
  eval_batch_size: 32
  learning_rate: 2.0e-5
  epochs: 3  # More epochs since no GPU speed
  warmup_steps: 100
  weight_decay: 0.01
  max_grad_norm: 1.0
  scheduler: "linear"
  optimizer: "adamw"
  gradient_accumulation_steps: 2  # Accumulate gradients for effective batch size 32
  fp16: false  # No mixed precision on CPU

data:
  dataset: "babe"
  train_split: 0.8
  preprocessing: "none"
  max_length: 256
  data_source: "gcs"
  filter_no_agreement: true

logging:
  tensorboard: true
  log_steps: 100
  eval_steps: 500
  save_steps: 1000
  log_confusion_matrix: true
  log_predictions: false

vertex:
  machine_type: "n1-standard-4"  # CPU-only machine
  # No accelerator fields - will run on CPU only
  disk_type: "pd-standard"
  disk_size_gb: 50
  preemptible: true  # Still 70% cost reduction
  max_replica_count: 1