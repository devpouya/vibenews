experiment_name: "roberta_strong_v1"
description: "RoBERTa-base for potentially better performance on bias detection"
tags: ["roberta", "performance", "strong"]

model:
  architecture: "roberta"
  model_name: "roberta-base"
  num_labels: 3
  dropout: 0.1
  freeze_layers: 0

training:
  batch_size: 16
  eval_batch_size: 64
  learning_rate: 1.5e-5  # Lower LR for RoBERTa
  epochs: 3
  warmup_steps: 600  # More warmup for RoBERTa
  weight_decay: 0.01
  max_grad_norm: 1.0
  scheduler: "linear"
  optimizer: "adamw"

data:
  dataset: "babe"
  train_split: 0.8
  preprocessing: "none"
  max_length: 512
  augmentation: false
  balance_classes: false
  filter_no_agreement: false

logging:
  tensorboard: true
  wandb: false
  log_steps: 100
  eval_steps: 500
  save_steps: 1000
  log_model_graph: true
  log_confusion_matrix: true

cloud:
  platform: "gcp"
  machine_type: "n1-standard-4"
  gpu_type: "nvidia-tesla-v100"
  gpu_count: 1
  preemptible: true
  region: "us-central1"
  zone: "us-central1-a"