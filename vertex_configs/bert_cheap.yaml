# Cheap BERT Configuration (~$0.80 total cost)
experiment_name: "bert_cheap"
description: "Cost-optimized BERT training with good performance"
tags: ["cheap", "bert", "balanced"]

model:
  architecture: "bert"
  model_name: "bert-base-uncased"
  num_labels: 3
  dropout: 0.1
  freeze_layers: 2  # Freeze some layers for speed

training:
  batch_size: 24  # Balanced batch size
  eval_batch_size: 48
  learning_rate: 2.5e-5
  epochs: 2  # Fewer epochs
  warmup_steps: 300
  weight_decay: 0.01
  max_grad_norm: 1.0
  scheduler: "linear"
  optimizer: "adamw"
  gradient_accumulation_steps: 1
  fp16: true

data:
  dataset: "babe"
  train_split: 0.8
  preprocessing: "none"
  max_length: 384  # Reasonable sequence length
  data_source: "gcs"
  filter_no_agreement: false

logging:
  tensorboard: true
  log_steps: 100
  eval_steps: 300
  save_steps: 600
  log_confusion_matrix: true
  log_predictions: true

vertex:
  machine_type: "e2-standard-4"  # Good balance of cost/performance
  accelerator_type: "NVIDIA_TESLA_T4"
  accelerator_count: 1
  disk_type: "pd-standard"
  disk_size_gb: 50
  preemptible: true
  max_replica_count: 1